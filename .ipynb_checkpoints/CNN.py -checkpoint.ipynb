{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy import sparse\n",
    "from IPython.display import Image\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For re-sizing\n",
    "from skimage.transform import resize\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"alexachung\", \"bucketlistjourney\", \"mariekondo\", \"jamieoliver\", \"gypsea_lust\", \n",
    "         \"kimkardashian\", \"tombrady\", \"ocasio2018\"]\n",
    "# filelist = os.listdir(path)\n",
    "followers = {}\n",
    "followers[\"alexachung\"] = float(3300000)\n",
    "followers[\"bucketlistjourney\"] = float(100000)\n",
    "followers[\"mariekondo\"] = float(3000000)\n",
    "followers[\"jamieoliver\"] = float(6700000)\n",
    "followers[\"gypsea_lust\"] = float(2100000)\n",
    "followers[\"kimkardashian\"] = float(138000000)\n",
    "followers[\"tombrady\"] = float(6100000)\n",
    "followers[\"ocasio2018\"] = float(3400000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 1080, 3)\n"
     ]
    }
   ],
   "source": [
    "image_arrays = []\n",
    "for p in paths:\n",
    "    filelist = os.listdir(p)\n",
    "    for file in filelist:\n",
    "        if str(file)[-1] != \"g\":\n",
    "            continue\n",
    "        pic = load_img(p + str(\"/\") + str(file))\n",
    "        arr = img_to_array(pic)\n",
    "        image_arrays.append((p, str(file), arr))\n",
    "print(image_arrays[0][2].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142\n",
      "81\n",
      "81\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "dataDict = {}\n",
    "for path in paths:\n",
    "    with open(path + str(\"/\") + path + str(\".json\")) as f:\n",
    "        data = json.load(f)\n",
    "    dataDict[path] = data\n",
    "\n",
    "likes = {}\n",
    "final_arr = []\n",
    "likes_array = []\n",
    "for img in image_arrays:\n",
    "    jsonData = dataDict[img[0]]\n",
    "    for i in range(len(jsonData['GraphImages'])):\n",
    "        if img[1] in jsonData['GraphImages'][i][\"display_url\"]:\n",
    "            likes[img[1]] = jsonData['GraphImages'][i][\"edge_media_preview_like\"][\"count\"]/followers[img[0]]\n",
    "            final_arr.append(img)\n",
    "            likes_array.append(jsonData['GraphImages'][i][\"edge_media_preview_like\"][\"count\"]/followers[img[0]])\n",
    "            \n",
    "\n",
    "print(len(image_arrays))\n",
    "print(len(likes))\n",
    "\n",
    "\n",
    "print(len(final_arr))\n",
    "print(len(likes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_values = []\n",
    "i_height = 256\n",
    "i_width = 256\n",
    "for img in final_arr:\n",
    "    image = np.asarray(img[2])\n",
    "    image = np.divide(image, float(255))\n",
    "    image = skimage.transform.resize(image, (i_height, i_width, 3), anti_aliasing=True, mode='reflect')\n",
    "    img_values.append(image.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[0.00726273 0.00525697 0.00325788 0.01992364 0.00349242 0.00330152\n",
      " 0.02612818 0.01230455 0.00345818 0.01527    0.01893    0.02101\n",
      " 0.01268    0.01648    0.01537    0.01586    0.0143     0.01516\n",
      " 0.005652   0.09890533 0.00689167 0.02318033 0.01667667 0.00846433\n",
      " 0.018406   0.013795   0.011268   0.02625333 0.00457478 0.00309254\n",
      " 0.00440985 0.00433552 0.00270791 0.00363343 0.02360238 0.02661571\n",
      " 0.02395476 0.02778381 0.01838286 0.03593381 0.02072476 0.02280381\n",
      " 0.02300667 0.03447238 0.01263476 0.01695762 0.00181449 0.01279488\n",
      " 0.02663065 0.01418512 0.0121562  0.02043403 0.02202417 0.0157633\n",
      " 0.01075553 0.04449575 0.02532577 0.03876755 0.00961025 0.01409393\n",
      " 0.0625359  0.11832869 0.05896213 0.09238492 0.08708574 0.03607459\n",
      " 0.05351656 0.05183902 0.0807677  0.06024459 0.07291098 0.06169623\n",
      " 0.07888574 0.06452618 0.14062147 0.21582735 0.08647588 0.09412882\n",
      " 0.03146059 0.06845324 0.24071853]\n",
      "[ 2  1  1  6  1  1  8  4  1  5  6  7  4  5  5  5  4  5  1 32  2  7  5  2\n",
      "  6  4  3  8  1  1  1  1  0  1  7  8  7  9  6 11  6  7  7 11  4  5  0  4\n",
      "  8  4  4  6  7  5  3 14  8 12  3  4 20 39 19 30 29 12 17 17 26 20 24 20\n",
      " 26 21 46 71 28 31 10 22 80]\n"
     ]
    }
   ],
   "source": [
    "# Make y discrete\n",
    "\n",
    "# y = np.asarray(likes)\n",
    "y = np.asarray(likes_array)\n",
    "print(type(y))\n",
    "new_y = np.divide(y, 0.003)\n",
    "print(y)\n",
    "new_y = new_y.astype(int)\n",
    "print(new_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(img_values, new_y, test_size=0.1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(nn.Module):\n",
    "    def forward(self, x):\n",
    "        # N, C, H, W = x.size() # read in N, C, H, W\n",
    "        return x.view(N, -1)  # \"flatten\" the C * H * W values into a single vector per image\n",
    "    \n",
    "class Unflatten(nn.Module):\n",
    "    \"\"\"\n",
    "    An Unflatten module receives an input of shape (N, C*H*W) and reshapes it\n",
    "    to produce an output of shape (N, C, H, W).\n",
    "    \"\"\"\n",
    "    def __init__(self, N=-1, C=128, H=7, W=7):\n",
    "        super(Unflatten, self).__init__()\n",
    "        self.N = N\n",
    "        self.C = C\n",
    "        self.H = H\n",
    "        self.W = W\n",
    "    def forward(self, x):\n",
    "        return x.view(self.N, self.C, self.H, self.W)\n",
    "\n",
    "def initialize_weights(m):\n",
    "    if isinstance(m, nn.Linear) or isinstance(m, nn.ConvTranspose2d):\n",
    "        init.xavier_uniform_(m.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildConvNet(input_dim):\n",
    "    return nn.Sequential(\n",
    "#         Flatten(),\n",
    "        nn.Linear(in_features=input_dim, out_features=1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(num_features=1024),\n",
    "        nn.Linear(in_features=1024, out_features=128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.BatchNorm1d(num_features=128),\n",
    "        Unflatten(1, 128,9,9),\n",
    "        nn.Conv2d(in_channels=128, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.Conv2d(in_channels=64, out_channels=32, kernel_size=4, stride=2, padding=1),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(in_features=2, out_features=1),\n",
    "        nn.Tanh(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196608,)\n",
      "tensor([[[[0.4748],\n",
      "          [0.4534]],\n",
      "\n",
      "         [[0.1381],\n",
      "          [0.2532]],\n",
      "\n",
      "         [[0.4260],\n",
      "          [0.3669]],\n",
      "\n",
      "         [[0.1897],\n",
      "          [0.4876]],\n",
      "\n",
      "         [[0.4149],\n",
      "          [0.3984]],\n",
      "\n",
      "         [[0.3289],\n",
      "          [0.3276]],\n",
      "\n",
      "         [[0.5189],\n",
      "          [0.4605]],\n",
      "\n",
      "         [[0.3003],\n",
      "          [0.2492]],\n",
      "\n",
      "         [[0.4036],\n",
      "          [0.4950]],\n",
      "\n",
      "         [[0.4647],\n",
      "          [0.1039]],\n",
      "\n",
      "         [[0.4575],\n",
      "          [0.2399]],\n",
      "\n",
      "         [[0.1436],\n",
      "          [0.4437]],\n",
      "\n",
      "         [[0.4865],\n",
      "          [0.4068]],\n",
      "\n",
      "         [[0.3071],\n",
      "          [0.3423]],\n",
      "\n",
      "         [[0.3654],\n",
      "          [0.4822]],\n",
      "\n",
      "         [[0.3418],\n",
      "          [0.3562]],\n",
      "\n",
      "         [[0.2731],\n",
      "          [0.2816]],\n",
      "\n",
      "         [[0.2218],\n",
      "          [0.3539]],\n",
      "\n",
      "         [[0.5539],\n",
      "          [0.2398]],\n",
      "\n",
      "         [[0.3136],\n",
      "          [0.4855]],\n",
      "\n",
      "         [[0.4874],\n",
      "          [0.3377]],\n",
      "\n",
      "         [[0.3559],\n",
      "          [0.4482]],\n",
      "\n",
      "         [[0.3385],\n",
      "          [0.5470]],\n",
      "\n",
      "         [[0.2823],\n",
      "          [0.5432]],\n",
      "\n",
      "         [[0.5199],\n",
      "          [0.1870]],\n",
      "\n",
      "         [[0.5803],\n",
      "          [0.4191]],\n",
      "\n",
      "         [[0.3760],\n",
      "          [0.3333]],\n",
      "\n",
      "         [[0.3384],\n",
      "          [0.3066]],\n",
      "\n",
      "         [[0.2469],\n",
      "          [0.3414]],\n",
      "\n",
      "         [[0.4615],\n",
      "          [0.3456]],\n",
      "\n",
      "         [[0.3121],\n",
      "          [0.1386]],\n",
      "\n",
      "         [[0.6279],\n",
      "          [0.2308]]]], grad_fn=<TanhBackward>)\n"
     ]
    }
   ],
   "source": [
    "test_conv = buildConvNet(img_values[0].shape[0]).type(torch.FloatTensor)\n",
    "test_conv.apply(initialize_weights)\n",
    "print(img_values[0].shape)\n",
    "# fake_seed = torch.randn(batch_size, NOISE_DIM).type(dtype) # print(batch_size, NOISE_DIM)\n",
    "# print(fake_seed.size())\n",
    "thing_1 = test_conv.forward(torch.Tensor(img_values))\n",
    "# fake_images.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(test_conv.parameters(), lr=1e-3)\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs=1):\n",
    "\n",
    "    model = model.to(device=device)  # move the model parameters to CPU/GPU\n",
    "    for e in range(epochs):\n",
    "        t = 0\n",
    "        for x, y in enumerate((X_train, y_train)):\n",
    "            model.train()  # put model to training mode\n",
    "            x = x.to(device=device, dtype=dtype)  # move to device, e.g. GPU\n",
    "            y = y.to(device=device, dtype=torch.long)\n",
    "\n",
    "            scores = model(x)\n",
    "            loss = F.cross_entropy(scores, y)\n",
    "\n",
    "            # Zero out all of the gradients for the variables which the optimizer\n",
    "            # will update.\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # This is the backwards pass: compute the gradient of the loss with\n",
    "            # respect to each  parameter of the model.\n",
    "            loss.backward()\n",
    "\n",
    "            # Actually update the parameters of the model using the gradients\n",
    "            # computed by the backwards pass.\n",
    "            optimizer.step()\n",
    "            t += 1\n",
    "            if t % print_every == 0:\n",
    "                print('Iteration %d, loss = %.4f' % (t, loss.item()))\n",
    "                check_accuracy_part34(loader_val, model)\n",
    "                print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-736eb8ad9db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_conv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-58-9fe78e1d61b8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, epochs)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# put model to training mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# move to device, e.g. GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "train(test_conv, optimizer, epochs=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
