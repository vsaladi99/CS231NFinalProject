{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import sparse\n",
    "from IPython.display import Image\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn import preprocessing\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For re-sizing\n",
    "from skimage.transform import resize\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "import mdptoolbox.example\n",
    "import cvlib as cv\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageStat\n",
    "import cv2\n",
    "from cvlib.object_detection import draw_bbox\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_colorfulness(image):\n",
    "    # split the image into its respective RGB components\n",
    "    (B, G, R) = cv2.split(image.astype(\"float\"))\n",
    " \n",
    "    # compute rg = R - G\n",
    "    rg = np.absolute(R - G)\n",
    " \n",
    "    # compute yb = 0.5 * (R + G) - B\n",
    "    yb = np.absolute(0.5 * (R + G) - B)\n",
    " \n",
    "    # compute the mean and standard deviation of both `rg` and `yb`\n",
    "    (rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
    "    (ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
    " \n",
    "    # combine the mean and standard deviations\n",
    "    stdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
    "    meanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
    " \n",
    "    # derive the \"colorfulness\" metric and return it\n",
    "    return stdRoot + (0.3 * meanRoot)\n",
    "\n",
    "def percieved_brightness(im_file):\n",
    "   im = Image.open(im_file)\n",
    "   stat = ImageStat.Stat(im)\n",
    "   r,g,b = stat.mean\n",
    "   return math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "def brightness(im_file):\n",
    "   im = Image.open(im_file).convert('L')\n",
    "   stat = ImageStat.Stat(im)\n",
    "   return stat.mean[0]\n",
    "\n",
    "def detect_genders(face, conf, img_filename):\n",
    "    # apply face detection\n",
    "    img = cv2.imread(img_filename)\n",
    "    #face, conf = cv.detect_face(img)\n",
    "    gender_array = []\n",
    "    # loop through detected faces\n",
    "    for f in face:\n",
    "\n",
    "        (startX,startY) = f[0],f[1]\n",
    "        (endX,endY) = f[2],f[3]\n",
    "\n",
    "        # draw rectangle over face\n",
    "        cv2.rectangle(img, (startX,startY), (endX,endY), (0,255,0), 2)\n",
    "\n",
    "        face_crop = np.copy(img[startY:endY, startX:endX])\n",
    "\n",
    "        # apply gender detection\n",
    "        (label, confidence) = cv.detect_gender(face_crop)\n",
    "\n",
    "        #print(confidence)\n",
    "        #print(label)\n",
    "\n",
    "        idx = np.argmax(confidence)\n",
    "        label = label[idx]\n",
    "        gender_array.append(label)\n",
    "    return gender_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"alexachung\", \"bucketlistjourney\", \"mariekondo\", \"jamieoliver\", \"gypsea_lust\", \n",
    "         \"kimkardashian\", \"tombrady\", \"ocasio2018\"]\n",
    "# filelist = os.listdir(path)\n",
    "followers = {}\n",
    "followers[\"alexachung\"] = float(3300000)\n",
    "followers[\"bucketlistjourney\"] = float(100000)\n",
    "followers[\"mariekondo\"] = float(3000000)\n",
    "followers[\"jamieoliver\"] = float(6700000)\n",
    "followers[\"gypsea_lust\"] = float(2100000)\n",
    "followers[\"kimkardashian\"] = float(138000000)\n",
    "followers[\"tombrady\"] = float(6100000)\n",
    "followers[\"ocasio2018\"] = float(3400000)\n",
    "\n",
    "df = pd.DataFrame(columns=['filename', 'followers', 'num_faces', 'percieved_brightness', \n",
    "                           'people', 'brightness', 'avg_r', 'avg_g', 'avg_b', 'likes_norm', 'num_female', 'num_male', 'likes' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrays = []\n",
    "for p in paths:\n",
    "    filelist = os.listdir(p)\n",
    "    for file in filelist:\n",
    "        if str(file)[-1] != \"g\":\n",
    "            continue\n",
    "        file_name = p + str(\"/\") + str(file)\n",
    "        pic = load_img(file_name)\n",
    "        arr = img_to_array(pic)\n",
    "        image_arrays.append((p, str(file), arr))\n",
    "        df = df.append({'filename' : file_name} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = {}\n",
    "for path in paths:\n",
    "    with open(path + str(\"/\") + path + str(\".json\")) as f:\n",
    "        data = json.load(f)\n",
    "    dataDict[path] = data\n",
    "\n",
    "likes = {}\n",
    "final_arr = []\n",
    "likes_array = []\n",
    "for img in image_arrays:\n",
    "    jsonData = dataDict[img[0]]\n",
    "    for i in range(len(jsonData['GraphImages'])):\n",
    "        if img[1] in jsonData['GraphImages'][i][\"display_url\"]:\n",
    "            \n",
    "            image_file_name = img[0] + str(\"/\") + img[1] \n",
    "            \n",
    "            image = Image.open(image_file_name, 'r')\n",
    "            image_array = img_to_array(image)\n",
    "            image_cv2 = cv2.imread(image_file_name)\n",
    "\n",
    "            # Add likes to the df\n",
    "            likes[img[1]] = jsonData['GraphImages'][i][\"edge_media_preview_like\"][\"count\"]\n",
    "            df.loc[df['filename'] == image_file_name,'likes_norm'] = jsonData['GraphImages'][i][\"edge_media_preview_like\"][\"count\"]/float(followers[img[0]])\n",
    "            df.loc[df['filename'] == image_file_name,'likes'] = jsonData['GraphImages'][i][\"edge_media_preview_like\"][\"count\"]\n",
    "\n",
    "            # Add number of faces \n",
    "            faces, confidences = cv.detect_face(image_cv2)\n",
    "            df.loc[df['filename'] == image_file_name, 'num_faces'] = len(confidences)\n",
    "\n",
    "            # Add objects\n",
    "            bbox, label, conf = cv.detect_common_objects(image_cv2)\n",
    "            #df.loc[df['filename'] == image_file_name, 'objects'] = pd.Series(label)\n",
    "            \n",
    "            # Add people\n",
    "            df.loc[df['filename'] == image_file_name, 'people'] = label.count('person')\n",
    "            \n",
    "            # Add color averages\n",
    "            means = cv2.mean(image_array)\n",
    "            df.loc[df['filename'] == image_file_name, 'avg_r'] = int(means[0])\n",
    "            df.loc[df['filename'] == image_file_name, 'avg_g'] = int(means[1])\n",
    "            df.loc[df['filename'] == image_file_name, 'avg_b'] = int(means[2])\n",
    "            \n",
    "            \n",
    "            #Add \"colorfulness\" metric\n",
    "            #df.loc[df['filename'] == image_file_name,'colorfulness'] = image_colorfulness(image_array)\n",
    "            \n",
    "            # Add brightness and percieved brightness\n",
    "            df.loc[df['filename'] == image_file_name, 'brightness'] = brightness(image_file_name)\n",
    "            df.loc[df['filename'] == image_file_name, 'percieved_brightness'] = percieved_brightness(image_file_name)\n",
    "            \n",
    "            \n",
    "            # Add followers\n",
    "            df.loc[df['filename'] == image_file_name, 'followers'] = followers[img[0]]\n",
    "            \n",
    "            #Extract gender ratio of photo. \n",
    "            gender_arary = detect_genders(faces, confidences, image_file_name)\n",
    "            \n",
    "            if(len(faces) > 0):\n",
    "                df.loc[df['filename'] == image_file_name, 'num_male'] = gender_arary.count('man')\n",
    "                df.loc[df['filename'] == image_file_name, 'num_female'] = gender_arary.count('woman')\n",
    "            else:\n",
    "                df.loc[df['filename'] == image_file_name, 'num_male'] = 0\n",
    "                df.loc[df['filename'] == image_file_name, 'num_female'] = 0\n",
    "                            \n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              filename followers num_faces  \\\n",
      "0    alexachung/57939604_1405275606280969_227927799...   3.3e+06         1   \n",
      "1    alexachung/57487966_302650897331937_5403985171...   3.3e+06         0   \n",
      "3    alexachung/58409673_1888377867934548_313100938...   3.3e+06         0   \n",
      "4    alexachung/57511927_446139556140698_1423052042...   3.3e+06         1   \n",
      "5    alexachung/58994690_801384873562945_4583796651...   3.3e+06         0   \n",
      "6    alexachung/57568586_144722209994647_2830766459...   3.3e+06         0   \n",
      "7    alexachung/58453539_318563102171701_5107544445...   3.3e+06         1   \n",
      "8    alexachung/57598467_109244776838216_8820137001...   3.3e+06         1   \n",
      "9    alexachung/59444228_294271814828332_6767495519...   3.3e+06         0   \n",
      "10   bucketlistjourney/59767226_464352457440775_604...    100000         0   \n",
      "11   bucketlistjourney/57303614_417134662419576_839...    100000         0   \n",
      "12   bucketlistjourney/56551834_425069051389781_109...    100000         0   \n",
      "13   bucketlistjourney/59121268_335244693724167_156...    100000         0   \n",
      "14   bucketlistjourney/57079872_417108132452805_721...    100000         0   \n",
      "15   bucketlistjourney/56723983_2406294326067674_56...    100000         0   \n",
      "16   bucketlistjourney/57488073_1698865903746235_50...    100000         0   \n",
      "17   bucketlistjourney/59653184_473330100071412_854...    100000         0   \n",
      "18   bucketlistjourney/55833113_101630534350977_109...    100000         0   \n",
      "19   mariekondo/59578280_135526804257525_6333761325...     3e+06         0   \n",
      "20   mariekondo/58761690_440356990056612_3929126082...     3e+06         1   \n",
      "21   mariekondo/58410871_2006268226148824_757526892...     3e+06         0   \n",
      "22   mariekondo/60020974_135693017599477_3289982818...     3e+06         0   \n",
      "23   mariekondo/57488299_116322816235392_5850182112...     3e+06         0   \n",
      "24   mariekondo/59395411_533315747197450_3553175472...     3e+06         0   \n",
      "25   mariekondo/58633288_409904143168033_5649104692...     3e+06         1   \n",
      "26   mariekondo/57156342_439041043332802_6453832952...     3e+06         0   \n",
      "27   mariekondo/58454153_164208737935546_2059782334...     3e+06         0   \n",
      "28   mariekondo/59168564_298866867672149_9098514675...     3e+06         1   \n",
      "31   jamieoliver/59561488_2297684337111108_79061638...   6.7e+06         0   \n",
      "32   jamieoliver/58408962_138605013867794_280615895...   6.7e+06         0   \n",
      "..                                                 ...       ...       ...   \n",
      "76   kimkardashian/59422551_135250284219889_4562238...  1.38e+08         1   \n",
      "82   kimkardashian/58737534_842575826094452_3307665...  1.38e+08         2   \n",
      "84   kimkardashian/59649403_1179364348854076_366928...  1.38e+08         2   \n",
      "89   kimkardashian/58858164_625870107893522_1687353...  1.38e+08         2   \n",
      "95   kimkardashian/58729924_2601211539949794_840105...  1.38e+08         1   \n",
      "96   kimkardashian/57840131_644723422642924_4839165...  1.38e+08         1   \n",
      "98   kimkardashian/57585167_168379430839445_2013752...  1.38e+08         0   \n",
      "102  kimkardashian/59945730_408707003062530_5156476...  1.38e+08         1   \n",
      "104  kimkardashian/57506321_145476603237413_3001811...  1.38e+08         1   \n",
      "113  tombrady/59406780_669215330180108_609581654392...   6.1e+06         2   \n",
      "114  tombrady/59435239_388908528621592_773496659915...   6.1e+06         2   \n",
      "116  tombrady/59720176_443704659756350_572323186949...   6.1e+06         1   \n",
      "117  tombrady/56770808_399294940654325_659398019373...   6.1e+06         1   \n",
      "119  tombrady/57239236_2151670575145621_87045967431...   6.1e+06         1   \n",
      "120  tombrady/59842952_2380013568987558_62835807736...   6.1e+06         0   \n",
      "121  tombrady/58049269_2287333518181908_26970918839...   6.1e+06         2   \n",
      "122  tombrady/57506533_294660571448999_657438808910...   6.1e+06         0   \n",
      "126  tombrady/59315577_132852484490159_807856855215...   6.1e+06         0   \n",
      "127  tombrady/58802493_2336407739747810_46282304674...   6.1e+06         1   \n",
      "128  tombrady/58410057_137808924002849_514712687517...   6.1e+06         1   \n",
      "129  tombrady/57257135_2089384198025710_69411801531...   6.1e+06         2   \n",
      "131  tombrady/56749587_1024803434371859_41935135232...   6.1e+06         1   \n",
      "132  ocasio2018/52572824_2048897855225268_566729145...   3.4e+06         1   \n",
      "134  ocasio2018/51759811_406233833477172_4333460932...   3.4e+06         0   \n",
      "135  ocasio2018/51895369_121069092322781_7650286371...   3.4e+06         0   \n",
      "136  ocasio2018/51221661_382051905980540_3817109374...   3.4e+06         1   \n",
      "137  ocasio2018/52736729_263180281279482_6627642289...   3.4e+06         1   \n",
      "138  ocasio2018/54247730_305082723516262_2849514262...   3.4e+06         0   \n",
      "140  ocasio2018/50691545_346000186004538_2945915006...   3.4e+06         2   \n",
      "141  ocasio2018/54512400_401860030614688_5047090010...   3.4e+06         1   \n",
      "\n",
      "    percieved_brightness people brightness     avg_r     avg_g     avg_b  \\\n",
      "0               -56.0698      1   -55.7429  -61.4815  -54.3827  -45.5679   \n",
      "1               0.267806      2 -0.0508603   6.51852 -0.382716  -16.5679   \n",
      "3                7.99257      1    6.40781   1.51852   12.6173  -12.5679   \n",
      "4               -1.33222      1   -1.24648   3.51852  -2.38272  -10.5679   \n",
      "5               -31.2694      0    -30.444  -31.4815  -31.3827  -21.5679   \n",
      "6               -75.6677      0   -75.5737  -75.4815  -75.3827  -76.5679   \n",
      "7               -43.7355      2   -43.2002  -39.4815  -45.3827  -42.5679   \n",
      "8               -33.9628      1   -34.5205  -34.4815  -32.3827  -44.5679   \n",
      "9               -48.1237      0   -53.2364   8.51852  -91.3827  -23.5679   \n",
      "10               10.9979      0    10.7548   8.51852   12.6173    5.4321   \n",
      "11              -13.5895      1   -14.7919  -19.4815  -9.38272  -28.5679   \n",
      "12               -35.859      5   -35.8398  -40.4815  -34.3827  -33.5679   \n",
      "13               33.1323      0    32.7788   28.5185   35.6173   29.4321   \n",
      "14               14.7583      0    14.2066   3.51852   18.6173   18.4321   \n",
      "15               2.28388      1    2.23485  0.518519   3.61728  0.432099   \n",
      "16               4.89836      0    4.82723  -2.48148   7.61728   11.4321   \n",
      "17              -14.8326      1   -16.2954  -19.4815  -11.3827  -34.5679   \n",
      "18              -36.4632      1    -36.917  -43.4815  -33.3827  -37.5679   \n",
      "19               70.8083      0    70.6125   75.5185   70.6173   57.4321   \n",
      "20               23.8397      1    24.4445   19.5185   24.6173   35.4321   \n",
      "21               76.6934      0    76.8055   73.5185   78.6173   76.4321   \n",
      "22              -30.2143      0   -30.0866  -39.4815  -27.3827  -18.5679   \n",
      "23               48.4753      0    48.4419   58.5185   46.6173   31.4321   \n",
      "24               43.5352      0    43.7928   47.5185   43.6173   38.4321   \n",
      "25               13.5455      0    14.0928   6.51852   15.6173   25.4321   \n",
      "26               65.2665      0    65.6734   59.5185   66.6173   77.4321   \n",
      "27               55.0108      0    55.2506   54.5185   55.6173   54.4321   \n",
      "28               73.3111      1    73.5677   68.5185   74.6173   80.4321   \n",
      "31               53.0846      0    53.7765   50.5185   53.6173   61.4321   \n",
      "32               61.1043      0    60.6147   61.5185   62.6173   48.4321   \n",
      "..                   ...    ...        ...       ...       ...       ...   \n",
      "76              -35.1193      2   -34.6181  -16.4815  -42.3827  -43.5679   \n",
      "82               18.7145      1    19.8232   41.5185   10.6173   10.4321   \n",
      "84              -40.9227      2   -40.3008  -40.4815  -41.3827  -34.5679   \n",
      "89              -40.0826      2   -39.2709  -33.4815  -42.3827  -36.5679   \n",
      "95              -5.96771      1   -5.20428   18.5185  -15.3827  -16.5679   \n",
      "96              -59.1786      3   -58.6051  -63.4815  -58.3827  -48.5679   \n",
      "98               27.6297      4    28.4312   32.5185   25.6173   30.4321   \n",
      "102             -13.0125      5   -12.4827  -11.4815  -13.3827  -10.5679   \n",
      "104             -21.2925      1   -20.6272  -13.4815  -23.3827  -23.5679   \n",
      "113             -20.1659      1    -19.475  -28.4815  -17.3827   -8.5679   \n",
      "114             -2.66332     11   -1.80734   16.5185  -9.38272   -9.5679   \n",
      "116             -25.4958      3   -26.0613  -30.4815  -22.3827  -31.5679   \n",
      "117             -8.26081      1   -10.1022  -24.4815  -1.38272  -17.5679   \n",
      "119             -8.83866      2   -9.72982  -17.4815  -4.38272  -15.5679   \n",
      "120             -43.2844      2   -43.8261  -17.4815  -53.3827  -63.5679   \n",
      "121             -31.1113      6   -30.2322  -24.4815  -33.3827  -26.5679   \n",
      "122               8.7479     12    9.22254  -3.48148   11.6173   30.4321   \n",
      "126             -23.0492     15   -25.4029  -54.4815  -14.3827   -7.5679   \n",
      "127             -11.3855      1    -12.185  -23.4815  -6.38272  -10.5679   \n",
      "128             -13.8587      1   -14.0353  -17.4815  -11.3827  -15.5679   \n",
      "129             -80.7532      2   -80.0023  -76.4815  -83.3827  -73.5679   \n",
      "131              19.6746      1    19.7989   19.5185   20.6173   16.4321   \n",
      "132             -40.0518      3   -39.9123  -41.4815  -39.3827  -39.5679   \n",
      "134             -43.2512      0   -43.2272  -54.4815  -40.3827  -29.5679   \n",
      "135              99.7538      0    100.409   91.5185   102.617   111.432   \n",
      "136             -49.8133      3   -49.5957  -44.4815  -51.3827  -55.5679   \n",
      "137             -60.3848      2   -60.0905  -57.4815  -61.3827  -62.5679   \n",
      "138              12.0874      5     12.595   14.5185   11.6173   13.4321   \n",
      "140             -37.7026     15   -37.4788  -25.4815  -41.3827  -47.5679   \n",
      "141              17.6185      1    18.9655   32.5185   12.6173   16.4321   \n",
      "\n",
      "     likes_norm num_female num_male    likes  \n",
      "0    0.00726273          1        0    23967  \n",
      "1    0.00525697          0        0    17348  \n",
      "3    0.00325788          0        0    10751  \n",
      "4     0.0199236          1        0    65748  \n",
      "5    0.00349242          0        0    11525  \n",
      "6    0.00330152          0        0    10895  \n",
      "7     0.0261282          1        0    86223  \n",
      "8     0.0123045          1        0    40605  \n",
      "9    0.00345818          0        0    11412  \n",
      "10      0.01527          0        0     1527  \n",
      "11      0.01893          0        0     1893  \n",
      "12      0.02101          0        0     2101  \n",
      "13      0.01268          0        0     1268  \n",
      "14      0.01648          0        0     1648  \n",
      "15      0.01537          0        0     1537  \n",
      "16      0.01586          0        0     1586  \n",
      "17       0.0143          0        0     1430  \n",
      "18      0.01516          0        0     1516  \n",
      "19     0.005652          0        0    16956  \n",
      "20    0.0989053          0        1   296716  \n",
      "21   0.00689167          0        0    20675  \n",
      "22    0.0231803          0        0    69541  \n",
      "23    0.0166767          0        0    50030  \n",
      "24   0.00846433          0        0    25393  \n",
      "25     0.018406          1        0    55218  \n",
      "26     0.013795          0        0    41385  \n",
      "27     0.011268          0        0    33804  \n",
      "28    0.0262533          1        0    78760  \n",
      "31   0.00457478          0        0    30651  \n",
      "32   0.00309254          0        0    20720  \n",
      "..          ...        ...      ...      ...  \n",
      "76     0.020434          1        0  2819896  \n",
      "82    0.0220242          2        0  3039335  \n",
      "84    0.0157633          1        1  2175335  \n",
      "89    0.0107555          1        1  1484263  \n",
      "95    0.0444958          1        0  6140414  \n",
      "96    0.0253258          1        0  3494956  \n",
      "98    0.0387676          0        0  5349922  \n",
      "102  0.00961025          0        1  1326214  \n",
      "104   0.0140939          1        0  1944963  \n",
      "113   0.0625359          0        2   381469  \n",
      "114    0.118329          1        1   721805  \n",
      "116   0.0589621          0        1   359669  \n",
      "117   0.0923849          0        1   563548  \n",
      "119   0.0870857          0        1   531223  \n",
      "120   0.0360746          0        0   220055  \n",
      "121   0.0535166          0        2   326451  \n",
      "122    0.051839          0        0   316218  \n",
      "126   0.0807677          0        0   492683  \n",
      "127   0.0602446          0        1   367492  \n",
      "128    0.072911          0        1   444757  \n",
      "129   0.0616962          0        2   376347  \n",
      "131   0.0788857          0        1   481203  \n",
      "132   0.0645262          0        1   219389  \n",
      "134    0.140621          0        0   478113  \n",
      "135    0.215827          0        0   733813  \n",
      "136   0.0864759          1        0   294018  \n",
      "137   0.0941288          1        0   320038  \n",
      "138   0.0314606          0        0   106966  \n",
      "140   0.0684532          0        2   232741  \n",
      "141    0.240719          1        0   818443  \n",
      "\n",
      "[81 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Normalize numerical cols in DF\n",
    "df['percieved_brightness'] = (df['percieved_brightness'] - df['percieved_brightness'].mean())\n",
    "df['brightness'] = (df['brightness'] - df['brightness'].mean())\n",
    "df['avg_r'] = (df['avg_r'] - df['avg_r'].mean())\n",
    "df['avg_g'] = (df['avg_g'] - df['avg_g'].mean())\n",
    "df['avg_b'] = (df['avg_b'] - df['avg_b'].mean())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              filename    followers  \\\n",
      "0    alexachung/57939604_1405275606280969_227927799...    3300000.0   \n",
      "1    alexachung/57487966_302650897331937_5403985171...    3300000.0   \n",
      "3    alexachung/58409673_1888377867934548_313100938...    3300000.0   \n",
      "4    alexachung/57511927_446139556140698_1423052042...    3300000.0   \n",
      "5    alexachung/58994690_801384873562945_4583796651...    3300000.0   \n",
      "6    alexachung/57568586_144722209994647_2830766459...    3300000.0   \n",
      "7    alexachung/58453539_318563102171701_5107544445...    3300000.0   \n",
      "8    alexachung/57598467_109244776838216_8820137001...    3300000.0   \n",
      "9    alexachung/59444228_294271814828332_6767495519...    3300000.0   \n",
      "10   bucketlistjourney/59767226_464352457440775_604...     100000.0   \n",
      "11   bucketlistjourney/57303614_417134662419576_839...     100000.0   \n",
      "12   bucketlistjourney/56551834_425069051389781_109...     100000.0   \n",
      "13   bucketlistjourney/59121268_335244693724167_156...     100000.0   \n",
      "14   bucketlistjourney/57079872_417108132452805_721...     100000.0   \n",
      "15   bucketlistjourney/56723983_2406294326067674_56...     100000.0   \n",
      "16   bucketlistjourney/57488073_1698865903746235_50...     100000.0   \n",
      "17   bucketlistjourney/59653184_473330100071412_854...     100000.0   \n",
      "18   bucketlistjourney/55833113_101630534350977_109...     100000.0   \n",
      "19   mariekondo/59578280_135526804257525_6333761325...    3000000.0   \n",
      "20   mariekondo/58761690_440356990056612_3929126082...    3000000.0   \n",
      "21   mariekondo/58410871_2006268226148824_757526892...    3000000.0   \n",
      "22   mariekondo/60020974_135693017599477_3289982818...    3000000.0   \n",
      "23   mariekondo/57488299_116322816235392_5850182112...    3000000.0   \n",
      "24   mariekondo/59395411_533315747197450_3553175472...    3000000.0   \n",
      "25   mariekondo/58633288_409904143168033_5649104692...    3000000.0   \n",
      "26   mariekondo/57156342_439041043332802_6453832952...    3000000.0   \n",
      "27   mariekondo/58454153_164208737935546_2059782334...    3000000.0   \n",
      "28   mariekondo/59168564_298866867672149_9098514675...    3000000.0   \n",
      "31   jamieoliver/59561488_2297684337111108_79061638...    6700000.0   \n",
      "32   jamieoliver/58408962_138605013867794_280615895...    6700000.0   \n",
      "..                                                 ...          ...   \n",
      "76   kimkardashian/59422551_135250284219889_4562238...  138000000.0   \n",
      "82   kimkardashian/58737534_842575826094452_3307665...  138000000.0   \n",
      "84   kimkardashian/59649403_1179364348854076_366928...  138000000.0   \n",
      "89   kimkardashian/58858164_625870107893522_1687353...  138000000.0   \n",
      "95   kimkardashian/58729924_2601211539949794_840105...  138000000.0   \n",
      "96   kimkardashian/57840131_644723422642924_4839165...  138000000.0   \n",
      "98   kimkardashian/57585167_168379430839445_2013752...  138000000.0   \n",
      "102  kimkardashian/59945730_408707003062530_5156476...  138000000.0   \n",
      "104  kimkardashian/57506321_145476603237413_3001811...  138000000.0   \n",
      "113  tombrady/59406780_669215330180108_609581654392...    6100000.0   \n",
      "114  tombrady/59435239_388908528621592_773496659915...    6100000.0   \n",
      "116  tombrady/59720176_443704659756350_572323186949...    6100000.0   \n",
      "117  tombrady/56770808_399294940654325_659398019373...    6100000.0   \n",
      "119  tombrady/57239236_2151670575145621_87045967431...    6100000.0   \n",
      "120  tombrady/59842952_2380013568987558_62835807736...    6100000.0   \n",
      "121  tombrady/58049269_2287333518181908_26970918839...    6100000.0   \n",
      "122  tombrady/57506533_294660571448999_657438808910...    6100000.0   \n",
      "126  tombrady/59315577_132852484490159_807856855215...    6100000.0   \n",
      "127  tombrady/58802493_2336407739747810_46282304674...    6100000.0   \n",
      "128  tombrady/58410057_137808924002849_514712687517...    6100000.0   \n",
      "129  tombrady/57257135_2089384198025710_69411801531...    6100000.0   \n",
      "131  tombrady/56749587_1024803434371859_41935135232...    6100000.0   \n",
      "132  ocasio2018/52572824_2048897855225268_566729145...    3400000.0   \n",
      "134  ocasio2018/51759811_406233833477172_4333460932...    3400000.0   \n",
      "135  ocasio2018/51895369_121069092322781_7650286371...    3400000.0   \n",
      "136  ocasio2018/51221661_382051905980540_3817109374...    3400000.0   \n",
      "137  ocasio2018/52736729_263180281279482_6627642289...    3400000.0   \n",
      "138  ocasio2018/54247730_305082723516262_2849514262...    3400000.0   \n",
      "140  ocasio2018/50691545_346000186004538_2945915006...    3400000.0   \n",
      "141  ocasio2018/54512400_401860030614688_5047090010...    3400000.0   \n",
      "\n",
      "     num_faces percieved_brightness  people brightness  likes_norm    likes  \\\n",
      "0            1             -56.0698       1   -55.7429  0.00726273    23967   \n",
      "1            0             0.267806       2 -0.0508603  0.00525697    17348   \n",
      "3            0              7.99257       1    6.40781  0.00325788    10751   \n",
      "4            1             -1.33222       1   -1.24648   0.0199236    65748   \n",
      "5            0             -31.2694       0    -30.444  0.00349242    11525   \n",
      "6            0             -75.6677       0   -75.5737  0.00330152    10895   \n",
      "7            1             -43.7355       2   -43.2002   0.0261282    86223   \n",
      "8            1             -33.9628       1   -34.5205   0.0123045    40605   \n",
      "9            0             -48.1237       0   -53.2364  0.00345818    11412   \n",
      "10           0              10.9979       0    10.7548     0.01527     1527   \n",
      "11           0             -13.5895       1   -14.7919     0.01893     1893   \n",
      "12           0              -35.859       5   -35.8398     0.02101     2101   \n",
      "13           0              33.1323       0    32.7788     0.01268     1268   \n",
      "14           0              14.7583       0    14.2066     0.01648     1648   \n",
      "15           0              2.28388       1    2.23485     0.01537     1537   \n",
      "16           0              4.89836       0    4.82723     0.01586     1586   \n",
      "17           0             -14.8326       1   -16.2954      0.0143     1430   \n",
      "18           0             -36.4632       1    -36.917     0.01516     1516   \n",
      "19           0              70.8083       0    70.6125    0.005652    16956   \n",
      "20           1              23.8397       1    24.4445   0.0989053   296716   \n",
      "21           0              76.6934       0    76.8055  0.00689167    20675   \n",
      "22           0             -30.2143       0   -30.0866   0.0231803    69541   \n",
      "23           0              48.4753       0    48.4419   0.0166767    50030   \n",
      "24           0              43.5352       0    43.7928  0.00846433    25393   \n",
      "25           1              13.5455       0    14.0928    0.018406    55218   \n",
      "26           0              65.2665       0    65.6734    0.013795    41385   \n",
      "27           0              55.0108       0    55.2506    0.011268    33804   \n",
      "28           1              73.3111       1    73.5677   0.0262533    78760   \n",
      "31           0              53.0846       0    53.7765  0.00457478    30651   \n",
      "32           0              61.1043       0    60.6147  0.00309254    20720   \n",
      "..         ...                  ...     ...        ...         ...      ...   \n",
      "76           1             -35.1193       2   -34.6181    0.020434  2819896   \n",
      "82           2              18.7145       1    19.8232   0.0220242  3039335   \n",
      "84           2             -40.9227       2   -40.3008   0.0157633  2175335   \n",
      "89           2             -40.0826       2   -39.2709   0.0107555  1484263   \n",
      "95           1             -5.96771       1   -5.20428   0.0444958  6140414   \n",
      "96           1             -59.1786       3   -58.6051   0.0253258  3494956   \n",
      "98           0              27.6297       4    28.4312   0.0387676  5349922   \n",
      "102          1             -13.0125       5   -12.4827  0.00961025  1326214   \n",
      "104          1             -21.2925       1   -20.6272   0.0140939  1944963   \n",
      "113          2             -20.1659       1    -19.475   0.0625359   381469   \n",
      "114          2             -2.66332      11   -1.80734    0.118329   721805   \n",
      "116          1             -25.4958       3   -26.0613   0.0589621   359669   \n",
      "117          1             -8.26081       1   -10.1022   0.0923849   563548   \n",
      "119          1             -8.83866       2   -9.72982   0.0870857   531223   \n",
      "120          0             -43.2844       2   -43.8261   0.0360746   220055   \n",
      "121          2             -31.1113       6   -30.2322   0.0535166   326451   \n",
      "122          0               8.7479      12    9.22254    0.051839   316218   \n",
      "126          0             -23.0492      15   -25.4029   0.0807677   492683   \n",
      "127          1             -11.3855       1    -12.185   0.0602446   367492   \n",
      "128          1             -13.8587       1   -14.0353    0.072911   444757   \n",
      "129          2             -80.7532       2   -80.0023   0.0616962   376347   \n",
      "131          1              19.6746       1    19.7989   0.0788857   481203   \n",
      "132          1             -40.0518       3   -39.9123   0.0645262   219389   \n",
      "134          0             -43.2512       0   -43.2272    0.140621   478113   \n",
      "135          0              99.7538       0    100.409    0.215827   733813   \n",
      "136          1             -49.8133       3   -49.5957   0.0864759   294018   \n",
      "137          1             -60.3848       2   -60.0905   0.0941288   320038   \n",
      "138          0              12.0874       5     12.595   0.0314606   106966   \n",
      "140          2             -37.7026      15   -37.4788   0.0684532   232741   \n",
      "141          1              17.6185       1    18.9655    0.240719   818443   \n",
      "\n",
      "         avg_r       avg_g       avg_b percieved_brightness_discrete  \\\n",
      "0   -61.481481  -54.382716  -45.567901                    (-60, -40]   \n",
      "1     6.518519   -0.382716  -16.567901                       (0, 20]   \n",
      "3     1.518519   12.617284  -12.567901                       (0, 20]   \n",
      "4     3.518519   -2.382716  -10.567901                      (-40, 0]   \n",
      "5   -31.481481  -31.382716  -21.567901                      (-40, 0]   \n",
      "6   -75.481481  -75.382716  -76.567901                   (-150, -60]   \n",
      "7   -39.481481  -45.382716  -42.567901                    (-60, -40]   \n",
      "8   -34.481481  -32.382716  -44.567901                      (-40, 0]   \n",
      "9     8.518519  -91.382716  -23.567901                    (-60, -40]   \n",
      "10    8.518519   12.617284    5.432099                       (0, 20]   \n",
      "11  -19.481481   -9.382716  -28.567901                      (-40, 0]   \n",
      "12  -40.481481  -34.382716  -33.567901                      (-40, 0]   \n",
      "13   28.518519   35.617284   29.432099                      (20, 40]   \n",
      "14    3.518519   18.617284   18.432099                       (0, 20]   \n",
      "15    0.518519    3.617284    0.432099                       (0, 20]   \n",
      "16   -2.481481    7.617284   11.432099                       (0, 20]   \n",
      "17  -19.481481  -11.382716  -34.567901                      (-40, 0]   \n",
      "18  -43.481481  -33.382716  -37.567901                      (-40, 0]   \n",
      "19   75.518519   70.617284   57.432099                     (60, 150]   \n",
      "20   19.518519   24.617284   35.432099                      (20, 40]   \n",
      "21   73.518519   78.617284   76.432099                     (60, 150]   \n",
      "22  -39.481481  -27.382716  -18.567901                      (-40, 0]   \n",
      "23   58.518519   46.617284   31.432099                      (40, 60]   \n",
      "24   47.518519   43.617284   38.432099                      (40, 60]   \n",
      "25    6.518519   15.617284   25.432099                       (0, 20]   \n",
      "26   59.518519   66.617284   77.432099                     (60, 150]   \n",
      "27   54.518519   55.617284   54.432099                      (40, 60]   \n",
      "28   68.518519   74.617284   80.432099                     (60, 150]   \n",
      "31   50.518519   53.617284   61.432099                      (40, 60]   \n",
      "32   61.518519   62.617284   48.432099                     (60, 150]   \n",
      "..         ...         ...         ...                           ...   \n",
      "76  -16.481481  -42.382716  -43.567901                      (-40, 0]   \n",
      "82   41.518519   10.617284   10.432099                       (0, 20]   \n",
      "84  -40.481481  -41.382716  -34.567901                    (-60, -40]   \n",
      "89  -33.481481  -42.382716  -36.567901                    (-60, -40]   \n",
      "95   18.518519  -15.382716  -16.567901                      (-40, 0]   \n",
      "96  -63.481481  -58.382716  -48.567901                    (-60, -40]   \n",
      "98   32.518519   25.617284   30.432099                      (20, 40]   \n",
      "102 -11.481481  -13.382716  -10.567901                      (-40, 0]   \n",
      "104 -13.481481  -23.382716  -23.567901                      (-40, 0]   \n",
      "113 -28.481481  -17.382716   -8.567901                      (-40, 0]   \n",
      "114  16.518519   -9.382716   -9.567901                      (-40, 0]   \n",
      "116 -30.481481  -22.382716  -31.567901                      (-40, 0]   \n",
      "117 -24.481481   -1.382716  -17.567901                      (-40, 0]   \n",
      "119 -17.481481   -4.382716  -15.567901                      (-40, 0]   \n",
      "120 -17.481481  -53.382716  -63.567901                    (-60, -40]   \n",
      "121 -24.481481  -33.382716  -26.567901                      (-40, 0]   \n",
      "122  -3.481481   11.617284   30.432099                       (0, 20]   \n",
      "126 -54.481481  -14.382716   -7.567901                      (-40, 0]   \n",
      "127 -23.481481   -6.382716  -10.567901                      (-40, 0]   \n",
      "128 -17.481481  -11.382716  -15.567901                      (-40, 0]   \n",
      "129 -76.481481  -83.382716  -73.567901                   (-150, -60]   \n",
      "131  19.518519   20.617284   16.432099                       (0, 20]   \n",
      "132 -41.481481  -39.382716  -39.567901                    (-60, -40]   \n",
      "134 -54.481481  -40.382716  -29.567901                    (-60, -40]   \n",
      "135  91.518519  102.617284  111.432099                     (60, 150]   \n",
      "136 -44.481481  -51.382716  -55.567901                    (-60, -40]   \n",
      "137 -57.481481  -61.382716  -62.567901                   (-150, -60]   \n",
      "138  14.518519   11.617284   13.432099                       (0, 20]   \n",
      "140 -25.481481  -41.382716  -47.567901                      (-40, 0]   \n",
      "141  32.518519   12.617284   16.432099                       (0, 20]   \n",
      "\n",
      "    brightness_discrete   avg_r_disc   avg_g_disc   avg_b_disc  \n",
      "0            (-60, -40]  (-150, -60]   (-60, -40]   (-60, -40]  \n",
      "1              (-40, 0]      (0, 20]     (-40, 0]     (-40, 0]  \n",
      "3               (0, 20]      (0, 20]      (0, 20]     (-40, 0]  \n",
      "4              (-40, 0]      (0, 20]     (-40, 0]     (-40, 0]  \n",
      "5              (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "6           (-150, -60]  (-150, -60]  (-150, -60]  (-150, -60]  \n",
      "7            (-60, -40]     (-40, 0]   (-60, -40]   (-60, -40]  \n",
      "8              (-40, 0]     (-40, 0]     (-40, 0]   (-60, -40]  \n",
      "9            (-60, -40]      (0, 20]  (-150, -60]     (-40, 0]  \n",
      "10              (0, 20]      (0, 20]      (0, 20]      (0, 20]  \n",
      "11             (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "12             (-40, 0]   (-60, -40]     (-40, 0]     (-40, 0]  \n",
      "13             (20, 40]     (20, 40]     (20, 40]     (20, 40]  \n",
      "14              (0, 20]      (0, 20]      (0, 20]      (0, 20]  \n",
      "15              (0, 20]      (0, 20]      (0, 20]      (0, 20]  \n",
      "16              (0, 20]     (-40, 0]      (0, 20]      (0, 20]  \n",
      "17             (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "18             (-40, 0]   (-60, -40]     (-40, 0]     (-40, 0]  \n",
      "19            (60, 150]    (60, 150]    (60, 150]     (40, 60]  \n",
      "20             (20, 40]      (0, 20]     (20, 40]     (20, 40]  \n",
      "21            (60, 150]    (60, 150]    (60, 150]    (60, 150]  \n",
      "22             (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "23             (40, 60]     (40, 60]     (40, 60]     (20, 40]  \n",
      "24             (40, 60]     (40, 60]     (40, 60]     (20, 40]  \n",
      "25              (0, 20]      (0, 20]      (0, 20]     (20, 40]  \n",
      "26            (60, 150]     (40, 60]    (60, 150]    (60, 150]  \n",
      "27             (40, 60]     (40, 60]     (40, 60]     (40, 60]  \n",
      "28            (60, 150]    (60, 150]    (60, 150]    (60, 150]  \n",
      "31             (40, 60]     (40, 60]     (40, 60]    (60, 150]  \n",
      "32            (60, 150]    (60, 150]    (60, 150]     (40, 60]  \n",
      "..                  ...          ...          ...          ...  \n",
      "76             (-40, 0]     (-40, 0]   (-60, -40]   (-60, -40]  \n",
      "82              (0, 20]     (40, 60]      (0, 20]      (0, 20]  \n",
      "84           (-60, -40]   (-60, -40]   (-60, -40]     (-40, 0]  \n",
      "89             (-40, 0]     (-40, 0]   (-60, -40]     (-40, 0]  \n",
      "95             (-40, 0]      (0, 20]     (-40, 0]     (-40, 0]  \n",
      "96           (-60, -40]  (-150, -60]   (-60, -40]   (-60, -40]  \n",
      "98             (20, 40]     (20, 40]     (20, 40]     (20, 40]  \n",
      "102            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "104            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "113            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "114            (-40, 0]      (0, 20]     (-40, 0]     (-40, 0]  \n",
      "116            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "117            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "119            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "120          (-60, -40]     (-40, 0]   (-60, -40]  (-150, -60]  \n",
      "121            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "122             (0, 20]     (-40, 0]      (0, 20]     (20, 40]  \n",
      "126            (-40, 0]   (-60, -40]     (-40, 0]     (-40, 0]  \n",
      "127            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "128            (-40, 0]     (-40, 0]     (-40, 0]     (-40, 0]  \n",
      "129         (-150, -60]  (-150, -60]  (-150, -60]  (-150, -60]  \n",
      "131             (0, 20]      (0, 20]     (20, 40]      (0, 20]  \n",
      "132            (-40, 0]   (-60, -40]     (-40, 0]     (-40, 0]  \n",
      "134          (-60, -40]   (-60, -40]   (-60, -40]     (-40, 0]  \n",
      "135           (60, 150]    (60, 150]    (60, 150]    (60, 150]  \n",
      "136          (-60, -40]   (-60, -40]   (-60, -40]   (-60, -40]  \n",
      "137         (-150, -60]   (-60, -40]  (-150, -60]  (-150, -60]  \n",
      "138             (0, 20]      (0, 20]      (0, 20]      (0, 20]  \n",
      "140            (-40, 0]     (-40, 0]   (-60, -40]   (-60, -40]  \n",
      "141             (0, 20]     (20, 40]      (0, 20]      (0, 20]  \n",
      "\n",
      "[81 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "category = pd.cut(df.percieved_brightness,[-150, -60, -40, -0, 20, 40, 60, 150])\n",
    "category = category.to_frame()\n",
    "category.columns = ['percieved_brightness_discrete']\n",
    "df_new = pd.concat([df,category],axis = 1)\n",
    "\n",
    "category = pd.cut(df.brightness,[-150, -60, -40, -0, 20, 40, 60, 150])\n",
    "category = category.to_frame()\n",
    "category.columns = ['brightness_discrete']\n",
    "df_new = pd.concat([df_new, category],axis = 1)\n",
    "\n",
    "category = pd.cut(df.avg_r,[-150, -60, -40, -0, 20, 40, 60, 150])\n",
    "category = category.to_frame()\n",
    "category.columns = ['avg_r_disc']\n",
    "df_new = pd.concat([df_new, category],axis = 1)\n",
    "\n",
    "category = pd.cut(df.avg_g,[-150, -60, -40, -0, 20, 40, 60, 150])\n",
    "category = category.to_frame()\n",
    "category.columns = ['avg_g_disc']\n",
    "df_new = pd.concat([df_new, category],axis = 1)\n",
    "\n",
    "category = pd.cut(df.avg_b,[-150, -60, -40, -0, 20, 40, 60, 150])\n",
    "category = category.to_frame()\n",
    "category.columns = ['avg_b_disc']\n",
    "df_new = pd.concat([df_new, category],axis = 1)\n",
    "\n",
    "#concatenate age and its bin\n",
    "print(df_new)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.027783809523809523 0.01537 0.09238491803278688 0.026615714285714284\n",
      " 0.06845323529411765 0.0033015151515151516 0.05183901639344262 0.01268\n",
      " 0.1406214705882353 0.06452617647058824 0.02532576811594203\n",
      " 0.09412882352941176 0.007262727272727273 0.01516 0.018406\n",
      " 0.05351655737704918 0.06253590163934426]\n",
      "[ 2  4 21  1  1  8  1  2  0  0  0 24  9  0  1  0  0  0  2  7  6  0  1  2\n",
      "  0  1  2  2  8  2  3  3  1  3  1  1  1  1  0  2  7  0  0  6  3  2  1  1\n",
      "  2  0  1  8  1  1  1  0  5  2  0 11  3  1  1  2]\n",
      "17.294117647058822\n",
      "[ 2  1  9  2  6  0  5  1 14  6  2  9  0  1  1  5  6]\n",
      "['gypsea_lust/57012713_342228239741992_3191860124595154034_n.jpg'\n",
      " 'bucketlistjourney/56723983_2406294326067674_5616514260652929922_n.jpg'\n",
      " 'tombrady/56770808_399294940654325_6593980193733087902_n.jpg'\n",
      " 'gypsea_lust/59423327_443314916240152_3266229492340529069_n.jpg'\n",
      " 'ocasio2018/50691545_346000186004538_2945915006050350194_n.jpg'\n",
      " 'alexachung/57568586_144722209994647_2830766459764292268_n.jpg'\n",
      " 'tombrady/57506533_294660571448999_6574388089106440941_n.jpg'\n",
      " 'bucketlistjourney/59121268_335244693724167_1568576480147494457_n.jpg'\n",
      " 'ocasio2018/51759811_406233833477172_4333460932033302449_n.jpg'\n",
      " 'ocasio2018/52572824_2048897855225268_5667291455298473407_n.jpg'\n",
      " 'kimkardashian/57840131_644723422642924_4839165834231245282_n.jpg'\n",
      " 'ocasio2018/52736729_263180281279482_6627642289666875199_n.jpg'\n",
      " 'alexachung/57939604_1405275606280969_2279277994967177982_n.jpg'\n",
      " 'bucketlistjourney/55833113_101630534350977_1094977877308446010_n.jpg'\n",
      " 'mariekondo/58633288_409904143168033_5649104692346330542_n.jpg'\n",
      " 'tombrady/58049269_2287333518181908_269709188394276843_n.jpg'\n",
      " 'tombrady/59406780_669215330180108_6095816543927486195_n.jpg']\n",
      "                                              filename followers num_faces  \\\n",
      "49   gypsea_lust/57012713_342228239741992_319186012...   2.1e+06         0   \n",
      "15   bucketlistjourney/56723983_2406294326067674_56...    100000         0   \n",
      "117  tombrady/56770808_399294940654325_659398019373...   6.1e+06         1   \n",
      "47   gypsea_lust/59423327_443314916240152_326622949...   2.1e+06         0   \n",
      "140  ocasio2018/50691545_346000186004538_2945915006...   3.4e+06         2   \n",
      "6    alexachung/57568586_144722209994647_2830766459...   3.3e+06         0   \n",
      "122  tombrady/57506533_294660571448999_657438808910...   6.1e+06         0   \n",
      "13   bucketlistjourney/59121268_335244693724167_156...    100000         0   \n",
      "134  ocasio2018/51759811_406233833477172_4333460932...   3.4e+06         0   \n",
      "132  ocasio2018/52572824_2048897855225268_566729145...   3.4e+06         1   \n",
      "96   kimkardashian/57840131_644723422642924_4839165...  1.38e+08         1   \n",
      "137  ocasio2018/52736729_263180281279482_6627642289...   3.4e+06         1   \n",
      "0    alexachung/57939604_1405275606280969_227927799...   3.3e+06         1   \n",
      "18   bucketlistjourney/55833113_101630534350977_109...    100000         0   \n",
      "25   mariekondo/58633288_409904143168033_5649104692...     3e+06         1   \n",
      "121  tombrady/58049269_2287333518181908_26970918839...   6.1e+06         2   \n",
      "113  tombrady/59406780_669215330180108_609581654392...   6.1e+06         2   \n",
      "\n",
      "    percieved_brightness people brightness     avg_r    avg_g     avg_b  \\\n",
      "49               11.2799      2    11.1427   3.51852  14.6173   15.4321   \n",
      "15               2.28388      1    2.23485  0.518519  3.61728  0.432099   \n",
      "117             -8.26081      1   -10.1022  -24.4815 -1.38272  -17.5679   \n",
      "47               42.1217      2    42.3756   47.5185  41.6173   35.4321   \n",
      "140             -37.7026     15   -37.4788  -25.4815 -41.3827  -47.5679   \n",
      "6               -75.6677      0   -75.5737  -75.4815 -75.3827  -76.5679   \n",
      "122               8.7479     12    9.22254  -3.48148  11.6173   30.4321   \n",
      "13               33.1323      0    32.7788   28.5185  35.6173   29.4321   \n",
      "134             -43.2512      0   -43.2272  -54.4815 -40.3827  -29.5679   \n",
      "132             -40.0518      3   -39.9123  -41.4815 -39.3827  -39.5679   \n",
      "96              -59.1786      3   -58.6051  -63.4815 -58.3827  -48.5679   \n",
      "137             -60.3848      2   -60.0905  -57.4815 -61.3827  -62.5679   \n",
      "0               -56.0698      1   -55.7429  -61.4815 -54.3827  -45.5679   \n",
      "18              -36.4632      1    -36.917  -43.4815 -33.3827  -37.5679   \n",
      "25               13.5455      0    14.0928   6.51852  15.6173   25.4321   \n",
      "121             -31.1113      6   -30.2322  -24.4815 -33.3827  -26.5679   \n",
      "113             -20.1659      1    -19.475  -28.4815 -17.3827   -8.5679   \n",
      "\n",
      "     likes_norm num_female num_male    likes  \n",
      "49    0.0277838          0        0    58346  \n",
      "15      0.01537          0        0     1537  \n",
      "117   0.0923849          0        1   563548  \n",
      "47    0.0266157          0        0    55893  \n",
      "140   0.0684532          0        2   232741  \n",
      "6    0.00330152          0        0    10895  \n",
      "122    0.051839          0        0   316218  \n",
      "13      0.01268          0        0     1268  \n",
      "134    0.140621          0        0   478113  \n",
      "132   0.0645262          0        1   219389  \n",
      "96    0.0253258          1        0  3494956  \n",
      "137   0.0941288          1        0   320038  \n",
      "0    0.00726273          1        0    23967  \n",
      "18      0.01516          0        0     1516  \n",
      "25     0.018406          1        0    55218  \n",
      "121   0.0535166          0        2   326451  \n",
      "113   0.0625359          0        2   381469  \n",
      "[2 0 6 3 8 6 2 0 6 5 6 6 6 6 9 2 9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/sklearn/linear_model/logistic.py:719: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "# LOG REGRESSION\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "X_train = train[['num_faces', 'percieved_brightness', 'brightness', 'people', 'avg_r', 'avg_g', 'avg_b', 'num_female', 'num_male',]]\n",
    "y_train = np.ravel(train[['likes_norm']])\n",
    "\n",
    "X_test = test[['num_faces', 'percieved_brightness', 'brightness', 'people', 'avg_r', 'avg_g', 'avg_b',  'num_female', 'num_male',]]\n",
    "y_test = np.ravel(test[['likes_norm']])\n",
    "\n",
    "print(y_test)\n",
    "\n",
    "y_train = np.multiply(y_train, 100)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "y_test = np.multiply(y_test, 100)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "           \n",
    "print(y_train)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(img_values, new_y, test_size=0.1, random_state=0)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=2000,\n",
    "                         class_weight='balanced', verbose=5).fit(X_train, y_train)\n",
    "\n",
    "y = clf.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(y_test, y))\n",
    "print(y_test)\n",
    "print(np.ravel(test['filename']))\n",
    "print(test)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              filename followers num_faces  \\\n",
      "49   gypsea_lust/57012713_342228239741992_319186012...   2.1e+06         0   \n",
      "15   bucketlistjourney/56723983_2406294326067674_56...    100000         0   \n",
      "117  tombrady/56770808_399294940654325_659398019373...   6.1e+06         1   \n",
      "47   gypsea_lust/59423327_443314916240152_326622949...   2.1e+06         0   \n",
      "140  ocasio2018/50691545_346000186004538_2945915006...   3.4e+06         2   \n",
      "6    alexachung/57568586_144722209994647_2830766459...   3.3e+06         0   \n",
      "122  tombrady/57506533_294660571448999_657438808910...   6.1e+06         0   \n",
      "13   bucketlistjourney/59121268_335244693724167_156...    100000         0   \n",
      "134  ocasio2018/51759811_406233833477172_4333460932...   3.4e+06         0   \n",
      "132  ocasio2018/52572824_2048897855225268_566729145...   3.4e+06         1   \n",
      "96   kimkardashian/57840131_644723422642924_4839165...  1.38e+08         1   \n",
      "137  ocasio2018/52736729_263180281279482_6627642289...   3.4e+06         1   \n",
      "0    alexachung/57939604_1405275606280969_227927799...   3.3e+06         1   \n",
      "18   bucketlistjourney/55833113_101630534350977_109...    100000         0   \n",
      "25   mariekondo/58633288_409904143168033_5649104692...     3e+06         1   \n",
      "121  tombrady/58049269_2287333518181908_26970918839...   6.1e+06         2   \n",
      "113  tombrady/59406780_669215330180108_609581654392...   6.1e+06         2   \n",
      "\n",
      "    percieved_brightness people brightness     avg_r    avg_g     avg_b  \\\n",
      "49               11.2799      2    11.1427   3.51852  14.6173   15.4321   \n",
      "15               2.28388      1    2.23485  0.518519  3.61728  0.432099   \n",
      "117             -8.26081      1   -10.1022  -24.4815 -1.38272  -17.5679   \n",
      "47               42.1217      2    42.3756   47.5185  41.6173   35.4321   \n",
      "140             -37.7026     15   -37.4788  -25.4815 -41.3827  -47.5679   \n",
      "6               -75.6677      0   -75.5737  -75.4815 -75.3827  -76.5679   \n",
      "122               8.7479     12    9.22254  -3.48148  11.6173   30.4321   \n",
      "13               33.1323      0    32.7788   28.5185  35.6173   29.4321   \n",
      "134             -43.2512      0   -43.2272  -54.4815 -40.3827  -29.5679   \n",
      "132             -40.0518      3   -39.9123  -41.4815 -39.3827  -39.5679   \n",
      "96              -59.1786      3   -58.6051  -63.4815 -58.3827  -48.5679   \n",
      "137             -60.3848      2   -60.0905  -57.4815 -61.3827  -62.5679   \n",
      "0               -56.0698      1   -55.7429  -61.4815 -54.3827  -45.5679   \n",
      "18              -36.4632      1    -36.917  -43.4815 -33.3827  -37.5679   \n",
      "25               13.5455      0    14.0928   6.51852  15.6173   25.4321   \n",
      "121             -31.1113      6   -30.2322  -24.4815 -33.3827  -26.5679   \n",
      "113             -20.1659      1    -19.475  -28.4815 -17.3827   -8.5679   \n",
      "\n",
      "     likes_norm num_female num_male    likes  \n",
      "49    0.0277838          0        0    58346  \n",
      "15      0.01537          0        0     1537  \n",
      "117   0.0923849          0        1   563548  \n",
      "47    0.0266157          0        0    55893  \n",
      "140   0.0684532          0        2   232741  \n",
      "6    0.00330152          0        0    10895  \n",
      "122    0.051839          0        0   316218  \n",
      "13      0.01268          0        0     1268  \n",
      "134    0.140621          0        0   478113  \n",
      "132   0.0645262          0        1   219389  \n",
      "96    0.0253258          1        0  3494956  \n",
      "137   0.0941288          1        0   320038  \n",
      "0    0.00726273          1        0    23967  \n",
      "18      0.01516          0        0     1516  \n",
      "25     0.018406          1        0    55218  \n",
      "121   0.0535166          0        2   326451  \n",
      "113   0.0625359          0        2   381469  \n",
      "filename                         tombrady/59406780_669215330180108_609581654392...\n",
      "followers                                                                  6.1e+06\n",
      "num_faces                                                                        2\n",
      "percieved_brightness                                                      -20.1659\n",
      "people                                                                           1\n",
      "brightness                                                                 -19.475\n",
      "likes_norm                                                               0.0625359\n",
      "likes                                                                       381469\n",
      "avg_r                                                                     -28.4815\n",
      "avg_g                                                                     -17.3827\n",
      "avg_b                                                                      -8.5679\n",
      "percieved_brightness_discrete                                             (-40, 0]\n",
      "brightness_discrete                                                       (-40, 0]\n",
      "avg_r_disc                                                                (-40, 0]\n",
      "avg_g_disc                                                                (-40, 0]\n",
      "avg_b_disc                                                                (-40, 0]\n",
      "Name: 113, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(test)\n",
    "print(df_new.loc[113])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gypsea_lust/55914433_2636214979727755_3426739042950481133_n.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gypsea_lust/55914433_2636214979727755_3426739042950481133_n.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q[observation] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q():\n",
    "    \n",
    "\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "    \"\"\"\n",
    "    Creates an epsilon-greedy policy based on a given Q-function and epsilon.\n",
    "    \n",
    "    Args:\n",
    "        Q: A dictionary that maps from state -> action-values.\n",
    "            Each value is a numpy array of length nA (see below)\n",
    "        epsilon: The probability to select a random action. Float between 0 and 1.\n",
    "        nA: Number of actions in the environment.\n",
    "    \n",
    "    Returns:\n",
    "        A function that takes the observation as an argument and returns\n",
    "        the probabilities for each action in the form of a numpy array of length nA.\n",
    "    \n",
    "    \"\"\"\n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "    \"\"\"\n",
    "    Q-Learning algorithm: Off-policy TD control. Finds the optimal greedy policy\n",
    "    while following an epsilon-greedy policy\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI environment.\n",
    "        num_episodes: Number of episodes to run for.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        alpha: TD learning rate.\n",
    "        epsilon: Chance to sample a random action. Float between 0 and 1.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple (Q, episode_lengths).\n",
    "        Q is the optimal action-value function, a dictionary mapping state -> action values.\n",
    "        stats is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    # The final action-value function.\n",
    "    # A nested dictionary that maps state -> (action -> action-value).\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    # Keeps track of useful statistics\n",
    "    stats = plotting.EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    # The policy we're following\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Reset the environment and pick the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        # One step in the environment\n",
    "        # total_reward = 0.0\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a stepgypsea_lust/55914433_2636214979727755_3426739042950481133_n.jpg\n",
    "            action_probs = policy(state) (THIS IS A\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # TD Update\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return Q, stats"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
