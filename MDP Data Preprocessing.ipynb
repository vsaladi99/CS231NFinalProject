{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from scipy import sparse\n",
    "from IPython.display import Image\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "sns.set(style=\"ticks\", color_codes=True)\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "#For re-sizing\n",
    "from skimage.transform import resize\n",
    "from scipy import misc\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "\n",
    "import mdptoolbox.example\n",
    "import cvlib as cv\n",
    "from matplotlib.pyplot import imshow\n",
    "from PIL import Image, ImageStat\n",
    "import cv2\n",
    "from cvlib.object_detection import draw_bbox\n",
    "\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_colorfulness(image):\n",
    "    # split the image into its respective RGB components\n",
    "    (B, G, R) = cv2.split(image.astype(\"float\"))\n",
    " \n",
    "    # compute rg = R - G\n",
    "    rg = np.absolute(R - G)\n",
    " \n",
    "    # compute yb = 0.5 * (R + G) - B\n",
    "    yb = np.absolute(0.5 * (R + G) - B)\n",
    " \n",
    "    # compute the mean and standard deviation of both `rg` and `yb`\n",
    "    (rbMean, rbStd) = (np.mean(rg), np.std(rg))\n",
    "    (ybMean, ybStd) = (np.mean(yb), np.std(yb))\n",
    " \n",
    "    # combine the mean and standard deviations\n",
    "    stdRoot = np.sqrt((rbStd ** 2) + (ybStd ** 2))\n",
    "    meanRoot = np.sqrt((rbMean ** 2) + (ybMean ** 2))\n",
    " \n",
    "    # derive the \"colorfulness\" metric and return it\n",
    "    return stdRoot + (0.3 * meanRoot)\n",
    "\n",
    "def percieved_brightness(im_file):\n",
    "   im = Image.open(im_file)\n",
    "   stat = ImageStat.Stat(im)\n",
    "   r,g,b = stat.mean\n",
    "   return math.sqrt(0.241*(r**2) + 0.691*(g**2) + 0.068*(b**2))\n",
    "\n",
    "def brightness(im_file):\n",
    "   im = Image.open(im_file).convert('L')\n",
    "   stat = ImageStat.Stat(im)\n",
    "   return stat.mean[0]\n",
    "\n",
    "def detect_genders(face, conf, img_filename):\n",
    "    # apply face detection\n",
    "    img = cv2.imread(img_filename)\n",
    "    #face, conf = cv.detect_face(img)\n",
    "    gender_array = []\n",
    "    # loop through detected faces\n",
    "    for f in face:\n",
    "\n",
    "        (startX,startY) = f[0],f[1]\n",
    "        (endX,endY) = f[2],f[3]\n",
    "\n",
    "        # draw rectangle over face\n",
    "        cv2.rectangle(img, (startX,startY), (endX,endY), (0,255,0), 2)\n",
    "\n",
    "        face_crop = np.copy(img[startY:endY, startX:endX])\n",
    "\n",
    "        # apply gender detection\n",
    "        (label, confidence) = cv.detect_gender(face_crop)\n",
    "\n",
    "        #print(confidence)\n",
    "        #print(label)\n",
    "\n",
    "        idx = np.argmax(confidence)\n",
    "        label = label[idx]\n",
    "        gender_array.append(label)\n",
    "    return gender_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = [\"alexachung\", \"bucketlistjourney\", \"mariekondo\", \"jamieoliver\", \"gypsea_lust\", \n",
    "         \"kimkardashian\", \"tombrady\", \"ocasio2018\"]\n",
    "# filelist = os.listdir(path)\n",
    "followers = {}\n",
    "followers[\"alexachung\"] = float(3300000)\n",
    "followers[\"bucketlistjourney\"] = float(100000)\n",
    "followers[\"mariekondo\"] = float(3000000)\n",
    "followers[\"jamieoliver\"] = float(6700000)\n",
    "followers[\"gypsea_lust\"] = float(2100000)\n",
    "followers[\"kimkardashian\"] = float(138000000)\n",
    "followers[\"tombrady\"] = float(6100000)\n",
    "followers[\"ocasio2018\"] = float(3400000)\n",
    "\n",
    "df = pd.DataFrame(columns=['filename', 'num_faces', 'percieved_brightness', \n",
    "                           'colorfulness', 'people', 'brightness', 'people', 'likes' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_arrays = []\n",
    "for p in paths:\n",
    "    filelist = os.listdir(p)\n",
    "    for file in filelist:\n",
    "        if str(file)[-1] != \"g\":\n",
    "            continue\n",
    "        file_name = p + str(\"/\") + str(file)\n",
    "        pic = load_img(file_name)\n",
    "        arr = img_to_array(pic)\n",
    "        image_arrays.append((p, str(file), arr))\n",
    "        df = df.append({'filename' : file_name} , ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDict = {}\n",
    "for path in paths:\n",
    "    with open(path + str(\"/\") + path + str(\".json\")) as f:\n",
    "        data = json.load(f)\n",
    "    dataDict[path] = data\n",
    "\n",
    "likes = {}\n",
    "final_arr = []\n",
    "likes_array = []\n",
    "for img in image_arrays:\n",
    "    jsonData = dataDict[img[0]]\n",
    "    for i in range(len(jsonData['GraphImages'])):\n",
    "        if img[1] in jsonData['GraphImages'][i][\"display_url\"]:\n",
    "            \n",
    "            image_file_name = img[0] + str(\"/\") + img[1] \n",
    "            \n",
    "            image = Image.open(image_file_name, 'r')\n",
    "            image_array = img_to_array(pic)\n",
    "            image_cv2 = cv2.imread(image_file_name)\n",
    "\n",
    "            # Add likes to the df\n",
    "            likes[img[1]] = jsonData['GraphImages'][i][\"edge_media_preview_like\"][\"count\"]\n",
    "            df.loc[df['filename'] == image_file_name,'likes'] = jsonData['GraphImages'][i][\"edge_media_preview_like\"][\"count\"]/float(followers[img[0]])\n",
    "            \n",
    "            # Add number of faces \n",
    "            faces, confidences = cv.detect_face(image_cv2)\n",
    "            df.loc[df['filename'] == image_file_name, 'num_faces'] = len(confidences)\n",
    "\n",
    "            # Add objects\n",
    "            bbox, label, conf = cv.detect_common_objects(image_cv2)\n",
    "            #df.loc[df['filename'] == image_file_name, 'objects'] = pd.Series(label)\n",
    "            \n",
    "            # Add people\n",
    "            df.loc[df['filename'] == image_file_name, 'people'] = label.count('person')\n",
    "            \n",
    "            #Add \"colorfulness\" metric\n",
    "            df.loc[df['filename'] == image_file_name,'colorfulness'] = image_colorfulness(image_array)\n",
    "            \n",
    "            # Add brightness and percieved brightness\n",
    "            df.loc[df['filename'] == image_file_name, 'brightness'] = brightness(image_file_name)\n",
    "            df.loc[df['filename'] == image_file_name, 'percieved_brightness'] = percieved_brightness(image_file_name)\n",
    "            \n",
    "            #Extract gender ratio of photo. \n",
    "            #gender_arary = detect_genders(faces, confidences, image_file_name)\n",
    "            \n",
    "            #if(len(faces) > 0):\n",
    "                #df.loc[df['filename'] == image_file_name, 'percent_male'] = gender_arary.count('male')/float(len(faces))\n",
    "                #df.loc[df['filename'] == image_file_name, 'percent_female'] = gender_arary.count('female')/float(len(faces))\n",
    "            \n",
    "        \n",
    "                            \n",
    "df = df.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              filename num_faces  \\\n",
      "0    alexachung/57939604_1405275606280969_227927799...         1   \n",
      "1    alexachung/57487966_302650897331937_5403985171...         0   \n",
      "3    alexachung/58409673_1888377867934548_313100938...         0   \n",
      "4    alexachung/57511927_446139556140698_1423052042...         1   \n",
      "5    alexachung/58994690_801384873562945_4583796651...         0   \n",
      "6    alexachung/57568586_144722209994647_2830766459...         0   \n",
      "7    alexachung/58453539_318563102171701_5107544445...         1   \n",
      "8    alexachung/57598467_109244776838216_8820137001...         1   \n",
      "9    alexachung/59444228_294271814828332_6767495519...         0   \n",
      "10   bucketlistjourney/59767226_464352457440775_604...         0   \n",
      "11   bucketlistjourney/57303614_417134662419576_839...         0   \n",
      "12   bucketlistjourney/56551834_425069051389781_109...         0   \n",
      "13   bucketlistjourney/59121268_335244693724167_156...         0   \n",
      "14   bucketlistjourney/57079872_417108132452805_721...         0   \n",
      "15   bucketlistjourney/56723983_2406294326067674_56...         0   \n",
      "16   bucketlistjourney/57488073_1698865903746235_50...         0   \n",
      "17   bucketlistjourney/59653184_473330100071412_854...         0   \n",
      "18   bucketlistjourney/55833113_101630534350977_109...         0   \n",
      "19   mariekondo/59578280_135526804257525_6333761325...         0   \n",
      "20   mariekondo/58761690_440356990056612_3929126082...         1   \n",
      "21   mariekondo/58410871_2006268226148824_757526892...         0   \n",
      "22   mariekondo/60020974_135693017599477_3289982818...         0   \n",
      "23   mariekondo/57488299_116322816235392_5850182112...         0   \n",
      "24   mariekondo/59395411_533315747197450_3553175472...         0   \n",
      "25   mariekondo/58633288_409904143168033_5649104692...         1   \n",
      "26   mariekondo/57156342_439041043332802_6453832952...         0   \n",
      "27   mariekondo/58454153_164208737935546_2059782334...         0   \n",
      "28   mariekondo/59168564_298866867672149_9098514675...         1   \n",
      "31   jamieoliver/59561488_2297684337111108_79061638...         0   \n",
      "32   jamieoliver/58408962_138605013867794_280615895...         0   \n",
      "..                                                 ...       ...   \n",
      "76   kimkardashian/59422551_135250284219889_4562238...         1   \n",
      "82   kimkardashian/58737534_842575826094452_3307665...         2   \n",
      "84   kimkardashian/59649403_1179364348854076_366928...         2   \n",
      "89   kimkardashian/58858164_625870107893522_1687353...         2   \n",
      "95   kimkardashian/58729924_2601211539949794_840105...         1   \n",
      "96   kimkardashian/57840131_644723422642924_4839165...         1   \n",
      "98   kimkardashian/57585167_168379430839445_2013752...         0   \n",
      "102  kimkardashian/59945730_408707003062530_5156476...         1   \n",
      "104  kimkardashian/57506321_145476603237413_3001811...         1   \n",
      "113  tombrady/59406780_669215330180108_609581654392...         2   \n",
      "114  tombrady/59435239_388908528621592_773496659915...         2   \n",
      "116  tombrady/59720176_443704659756350_572323186949...         1   \n",
      "117  tombrady/56770808_399294940654325_659398019373...         1   \n",
      "119  tombrady/57239236_2151670575145621_87045967431...         1   \n",
      "120  tombrady/59842952_2380013568987558_62835807736...         0   \n",
      "121  tombrady/58049269_2287333518181908_26970918839...         2   \n",
      "122  tombrady/57506533_294660571448999_657438808910...         0   \n",
      "126  tombrady/59315577_132852484490159_807856855215...         0   \n",
      "127  tombrady/58802493_2336407739747810_46282304674...         1   \n",
      "128  tombrady/58410057_137808924002849_514712687517...         1   \n",
      "129  tombrady/57257135_2089384198025710_69411801531...         2   \n",
      "131  tombrady/56749587_1024803434371859_41935135232...         1   \n",
      "132  ocasio2018/52572824_2048897855225268_566729145...         1   \n",
      "134  ocasio2018/51759811_406233833477172_4333460932...         0   \n",
      "135  ocasio2018/51895369_121069092322781_7650286371...         0   \n",
      "136  ocasio2018/51221661_382051905980540_3817109374...         1   \n",
      "137  ocasio2018/52736729_263180281279482_6627642289...         1   \n",
      "138  ocasio2018/54247730_305082723516262_2849514262...         0   \n",
      "140  ocasio2018/50691545_346000186004538_2945915006...         2   \n",
      "141  ocasio2018/54512400_401860030614688_5047090010...         1   \n",
      "\n",
      "    percieved_brightness colorfulness people brightness people       likes  \n",
      "0                76.3178      72.3574      1    75.9538      1  0.00726273  \n",
      "1                132.655      72.3574      2    131.646      2  0.00525697  \n",
      "3                 140.38      72.3574      1    138.105      1  0.00325788  \n",
      "4                131.055      72.3574      1     130.45      1   0.0199236  \n",
      "5                101.118      72.3574      0    101.253      0  0.00349242  \n",
      "6                56.7199      72.3574      0    56.1231      0  0.00330152  \n",
      "7                88.6522      72.3574      2    88.4966      2   0.0261282  \n",
      "8                98.4249      72.3574      1    97.1763      1   0.0123045  \n",
      "9                84.2639      72.3574      0    78.4604      0  0.00345818  \n",
      "10               143.386      72.3574      0    142.452      0     0.01527  \n",
      "11               118.798      72.3574      1    116.905      1     0.01893  \n",
      "12               96.5286      72.3574      5     95.857      5     0.02101  \n",
      "13                165.52      72.3574      0    164.476      0     0.01268  \n",
      "14               147.146      72.3574      0    145.903      0     0.01648  \n",
      "15               134.672      72.3574      1    133.932      1     0.01537  \n",
      "16               137.286      72.3574      0    136.524      0     0.01586  \n",
      "17               117.555      72.3574      1    115.401      1      0.0143  \n",
      "18               95.9244      72.3574      1    94.7798      1     0.01516  \n",
      "19               203.196      72.3574      0    202.309      0    0.005652  \n",
      "20               156.227      72.3574      1    156.141      1   0.0989053  \n",
      "21               209.081      72.3574      0    208.502      0  0.00689167  \n",
      "22               102.173      72.3574      0     101.61      0   0.0231803  \n",
      "23               180.863      72.3574      0    180.139      0   0.0166767  \n",
      "24               175.923      72.3574      0     175.49      0  0.00846433  \n",
      "25               145.933      72.3574      0     145.79      0    0.018406  \n",
      "26               197.654      72.3574      0     197.37      0    0.013795  \n",
      "27               187.398      72.3574      0    186.947      0    0.011268  \n",
      "28               205.699      72.3574      1    205.264      1   0.0262533  \n",
      "31               185.472      72.3574      0    185.473      0  0.00457478  \n",
      "32               193.492      72.3574      0    192.311      0  0.00309254  \n",
      "..                   ...          ...    ...        ...    ...         ...  \n",
      "76               97.2684      72.3574      2    97.0787      2    0.020434  \n",
      "82               151.102      72.3574      1     151.52      1   0.0220242  \n",
      "84               91.4649      72.3574      2    91.3959      2   0.0157633  \n",
      "89                92.305      72.3574      2    92.4259      2   0.0107555  \n",
      "95                126.42      72.3574      1    126.492      1   0.0444958  \n",
      "96                73.209      72.3574      3    73.0917      3   0.0253258  \n",
      "98               160.017      72.3574      4    160.128      4   0.0387676  \n",
      "102              119.375      72.3574      5    119.214      5  0.00961025  \n",
      "104              111.095      72.3574      1     111.07      1   0.0140939  \n",
      "113              112.222      72.3574      1    112.222      1   0.0625359  \n",
      "114              129.724      72.3574     11    129.889     11    0.118329  \n",
      "116              106.892      72.3574      3    105.635      3   0.0589621  \n",
      "117              124.127      72.3574      1    121.595      1   0.0923849  \n",
      "119              123.549      72.3574      2    121.967      2   0.0870857  \n",
      "120              89.1032      72.3574      2    87.8707      2   0.0360746  \n",
      "121              101.276      72.3574      6    101.465      6   0.0535166  \n",
      "122              141.136      72.3574     12    140.919     12    0.051839  \n",
      "126              109.338      72.3574     15    106.294     15   0.0807677  \n",
      "127              121.002      72.3574      1    119.512      1   0.0602446  \n",
      "128              118.529      72.3574      1    117.662      1    0.072911  \n",
      "129              51.6344      72.3574      2    51.6945      2   0.0616962  \n",
      "131              152.062      72.3574      1    151.496      1   0.0788857  \n",
      "132              92.3358      72.3574      3    91.7845      3   0.0645262  \n",
      "134              89.1365      72.3574      0    88.4696      0    0.140621  \n",
      "135              232.141      72.3574      0    232.106      0    0.215827  \n",
      "136              82.5743      72.3574      3    82.1011      3   0.0864759  \n",
      "137              72.0028      72.3574      2    71.6063      2   0.0941288  \n",
      "138              144.475      72.3574      5    144.292      5   0.0314606  \n",
      "140              94.6851      72.3574     15    94.2179     15   0.0684532  \n",
      "141              150.006      72.3574      1    150.662      1    0.240719  \n",
      "\n",
      "[81 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[17  1  4  5 12  3  6  1  5  4  7  1  4  6  7  5  5  4  2 26 12 20  4 24\n",
      "  2  1  8  7  1  1 30  1  7  1  7  4  3 21 10  0  5 32  6 31  2  4  8  6\n",
      "  7  5  9  1 28 80  4  4 39 29 19 11 22  8 11  8]\n",
      "1128.7058823529412\n",
      "[20  7 20 17 71  0 14  8  1  1 46  6  3  5  5  6 26]\n",
      "[30 80 24 39 11 31 80 39  2 30  1  9 24  1  1  2 22]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "# LOG REGRESSION\n",
    "\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "\n",
    "X_train = train[['num_faces', 'percieved_brightness', \n",
    "                           'colorfulness', 'people', 'brightness', 'people']]\n",
    "y_train = np.ravel(train[['likes']])\n",
    "\n",
    "X_test = test[['num_faces', 'percieved_brightness', \n",
    "                           'colorfulness', 'people', 'brightness', 'people']]\n",
    "y_test = np.ravel(test[['likes']])\n",
    "\n",
    "y_train = np.divide(y_train, 0.003)\n",
    "y_train = y_train.astype(int)\n",
    "\n",
    "y_test = np.divide(y_test, 0.003)\n",
    "y_test = y_test.astype(int)\n",
    "\n",
    "           \n",
    "print(y_train)\n",
    "#X_train, X_test, y_train, y_test = train_test_split(img_values, new_y, test_size=0.1, random_state=0)\n",
    "\n",
    "clf = LogisticRegression(random_state=0, solver='lbfgs', multi_class='multinomial', max_iter=2000,\n",
    "                         class_weight='balanced', verbose=5).fit(X_train, y_train)\n",
    "\n",
    "y = clf.predict(X_test)\n",
    "\n",
    "print(mean_squared_error(y_test, y))\n",
    "print(y_test)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
